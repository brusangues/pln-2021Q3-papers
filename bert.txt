BERT {#sec:bert}
====

We introduce BERT and its detailed implementation in this section. There
are two steps in our framework: *pre-training* and *fine-tuning*. During
pre-training, the model is trained on unlabeled data over different
pre-training tasks. For fine-tuning, the BERT model is first initialized
with the pre-trained parameters, and all of the parameters are
fine-tuned using labeled data from the downstream tasks. Each downstream
task has separate fine-tuned models, even though they are initialized
with the same pre-trained parameters. The question-answering example in
Figure [\[fig:bert\_overall\]](#fig:bert_overall){reference-type="ref"
reference="fig:bert_overall"} will serve as a running example for this
section.

A distinctive feature of BERT is its unified architecture across
different tasks. There is minimal difference between the pre-trained
architecture and the final downstream architecture.

#### Model Architecture

BERT's model architecture is a multi-layer bidirectional Transformer
encoder based on the original implementation described in
@vaswani-etal:2017:_atten and released in the `tensor2tensor`
library.[^1] Because the use of Transformers has become common and our
implementation is almost identical to the original, we will omit an
exhaustive background description of the model architecture and refer
readers to @vaswani-etal:2017:_atten as well as excellent guides such as
"The Annotated Transformer."[^2]

In this work, we denote the number of layers (i.e., Transformer blocks)
as $L$, the hidden size as $H$, and the number of self-attention heads
as $A$.[^3] We primarily report results on two model sizes: (L=12,
H=768, A=12, Total Parameters=110M) and (L=24, H=1024, A=16, Total
Parameters=340M).

was chosen to have the same model size as OpenAI GPT for comparison
purposes. Critically, however, the BERT Transformer uses bidirectional
self-attention, while the GPT Transformer uses constrained
self-attention where every token can only attend to context to its
left.[^4]

#### Input/Output Representations

To make BERT handle a variety of down-stream tasks, our input
representation is able to unambiguously represent both a single sentence
and a pair of sentences (e.g., $\langle$ Question, Answer $\rangle$) in
one token sequence. Throughout this work, a "sentence" can be an
arbitrary span of contiguous text, rather than an actual linguistic
sentence. A "sequence" refers to the input token sequence to BERT, which
may be a single sentence or two sentences packed together.

We use WordPiece embeddings [@wu-etal:2016:_googl] with a 30,000 token
vocabulary. The first token of every sequence is always a special
classification token (`[CLS]`). The final hidden state corresponding to
this token is used as the aggregate sequence representation for
classification tasks. Sentence pairs are packed together into a single
sequence. We differentiate the sentences in two ways. First, we separate
them with a special token (`[SEP]`). Second, we add a learned embedding
to every token indicating whether it belongs to sentence `A` or sentence
`B`. As shown in
Figure [\[fig:bert\_overall\]](#fig:bert_overall){reference-type="ref"
reference="fig:bert_overall"}, we denote input embedding as $E$, the
final hidden vector of the special `[CLS]` token as
$C \in \mathbb{R}^{H}$, and the final hidden vector for the $i^{\rm th}$
input token as $T_i \in \mathbb{R}^H$.

For a given token, its input representation is constructed by summing
the corresponding token, segment, and position embeddings. A
visualization of this construction can be seen in
Figure [\[fig:input\_embeddings\]](#fig:input_embeddings){reference-type="ref"
reference="fig:input_embeddings"}.

![image](Input_Emebeddings.pdf){width="360px"}

Pre-training BERT {#sec:pretraining_tasks}
-----------------

Unlike @peters-etal:2018:_deep and @radford-etal:2018, we do not use
traditional left-to-right or right-to-left language models to pre-train
BERT. Instead, we pre-train BERT using two unsupervised tasks, described
in this section. This step is presented in the left part of
Figure [\[fig:bert\_overall\]](#fig:bert_overall){reference-type="ref"
reference="fig:bert_overall"}.

#### Task \#1: Masked LM

Intuitively, it is reasonable to believe that a deep bidirectional model
is strictly more powerful than either a left-to-right model or the
shallow concatenation of a left-to-right and a right-to-left model.
Unfortunately, standard conditional language models can only be trained
left-to-right *or* right-to-left, since bidirectional conditioning would
allow each word to indirectly "see itself", and the model could
trivially predict the target word in a multi-layered context.

In order to train a deep bidirectional representation, we simply mask
some percentage of the input tokens at random, and then predict those
masked tokens. We refer to this procedure as a "masked LM" (MLM),
although it is often referred to as a *Cloze* task in the
literature [@taylor:1953:_cloze]. In this case, the final hidden vectors
corresponding to the mask tokens are fed into an output softmax over the
vocabulary, as in a standard LM. In all of our experiments, we mask 15%
of all WordPiece tokens in each sequence at random. In contrast to
denoising auto-encoders [@vincent:2008], we only predict the masked
words rather than reconstructing the entire input.

Although this allows us to obtain a bidirectional pre-trained model, a
downside is that we are creating a mismatch between pre-training and
fine-tuning, since the `[MASK]` token does not appear during
fine-tuning. To mitigate this, we do not always replace "masked" words
with the actual `[MASK]` token. The training data generator chooses 15%
of the token positions at random for prediction. If the $i$-th token is
chosen, we replace the $i$-th token with (1) the `[MASK]` token 80% of
the time (2) a random token 10% of the time (3) the unchanged $i$-th
token 10% of the time. Then, $T_i$ will be used to predict the original
token with cross entropy loss. We compare variations of this procedure
in
Appendix [\[appendix:sec:different\_masks\]](#appendix:sec:different_masks){reference-type="ref"
reference="appendix:sec:different_masks"}.

#### Task \#2: Next Sentence Prediction (NSP)

Many important downstream tasks such as Question Answering (QA) and
Natural Language Inference (NLI) are based on understanding the
*relationship* between two sentences, which is not directly captured by
language modeling. In order to train a model that understands sentence
relationships, we pre-train for a binarized *next sentence prediction*
task that can be trivially generated from any monolingual corpus.
Specifically, when choosing the sentences `A` and `B` for each
pre-training example, 50% of the time `B` is the actual next sentence
that follows `A` (labeled as `IsNext`), and 50% of the time it is a
random sentence from the corpus (labeled as `NotNext`). As we show in
Figure [\[fig:bert\_overall\]](#fig:bert_overall){reference-type="ref"
reference="fig:bert_overall"}, $C$ is used for next sentence prediction
(NSP).[^5] Despite its simplicity, we demonstrate in
Section [\[sec:task\_ablation\]](#sec:task_ablation){reference-type="ref"
reference="sec:task_ablation"} that pre-training towards this task is
very beneficial to both QA and NLI. [^6] The NSP task is closely related
to representation-learning objectives used in
@DBLP:journals/corr/JerniteBS17 and @logeswaran2018an. However, in prior
work, only sentence embeddings are transferred to down-stream tasks,
where BERT transfers all parameters to initialize end-task model
parameters.

**Pre-training data** The pre-training procedure largely follows the
existing literature on language model pre-training. For the pre-training
corpus we use the BooksCorpus (800M words) [@zhu:2015] and English
Wikipedia (2,500M words). For Wikipedia we extract only the text
passages and ignore lists, tables, and headers. It is critical to use a
document-level corpus rather than a shuffled sentence-level corpus such
as the Billion Word Benchmark [@chelba-etal:2013:_one] in order to
extract long contiguous sequences.

Fine-tuning BERT {#sec:finetuning_procedure}
----------------

Fine-tuning is straightforward since the self-attention mechanism in the
Transformer allows BERT to model many downstream tasks---whether they
involve single text or text pairs---by swapping out the appropriate
inputs and outputs. For applications involving text pairs, a common
pattern is to independently encode text pairs before applying
bidirectional cross attention, such as . BERT instead uses the
self-attention mechanism to unify these two stages, as encoding a
concatenated text pair with self-attention effectively includes
*bidirectional* cross attention between two sentences.

For each task, we simply plug in the task-specific inputs and outputs
into BERT and fine-tune all the parameters end-to-end. At the input,
sentence `A` and sentence `B` from pre-training are analogous to (1)
sentence pairs in paraphrasing, (2) hypothesis-premise pairs in
entailment, (3) question-passage pairs in question answering, and (4) a
degenerate text-$\varnothing$ pair in text classification or sequence
tagging. At the output, the token representations are fed into an output
layer for token-level tasks, such as sequence tagging or question
answering, and the `[CLS]` representation is fed into an output layer
for classification, such as entailment or sentiment analysis.

Compared to pre-training, fine-tuning is relatively inexpensive. All of
the results in the paper can be replicated in at most 1 hour on a single
Cloud TPU, or a few hours on a GPU, starting from the exact same
pre-trained model.[^7] We describe the task-specific details in the
corresponding subsections of
Section [\[sec:experiments\]](#sec:experiments){reference-type="ref"
reference="sec:experiments"}. More details can be found in
Appendix [\[appendix:sec:fine\_tune\_details\_and\_figures\]](#appendix:sec:fine_tune_details_and_figures){reference-type="ref"
reference="appendix:sec:fine_tune_details_and_figures"}.

[^1]: https://github.com/tensorflow/tensor2tensor

[^2]: http://nlp.seas.harvard.edu/2018/04/03/attention.html

[^3]: In all cases we set the feed-forward/filter size to be $4H$, i.e.,
    3072 for the $H=768$ and 4096 for the $H=1024$.

[^4]: We note that in the literature the bidirectional Transformer is
    often referred to as a "Transformer encoder" while the
    left-context-only version is referred to as a "Transformer decoder"
    since it can be used for text generation.

[^5]: The final model achieves 97%-98% accuracy on NSP.

[^6]: The vector $C$ is not a meaningful sentence representation without
    fine-tuning, since it was trained with NSP.

[^7]: For example, the BERT SQuAD model can be trained in around 30
    minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%.
